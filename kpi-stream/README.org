* Overview

This project contains a webserver which can start Typhoon HIL simulations, stream KPIs
from them, load batch data, and launch batch data processing.

There are 3 major dependencies for this project:
1. InfluxDB
2. Grafana
3. Typhoon HIL

You _must_ setup at least InfluxDB and Typhoon HIL to get the system
working. Grafana is optional (for viewing the data).

* Installation
Install the requirements in ~requirements.txt~ by running:

#+BEGIN_SRC shell
pip3 install -r requirements.txt
#+END_SRC

Next you will have to install and configure Typhoon HIL, InfluxDB, and
Grafana.

** Typhoon HIL
First download "Typhoon HIL Control Center" from the Typhoon HIL website:
https://www.typhoon-hil.com/products/hil-software/

You can make sure Typhoon works properly by opening one of the ~.cpd~
files (called ``model files'') in the ~/typhoon~ directory of this
repository. You can run the models by selecting to use a virtual
HIL. Note that the Python webserver will be running the model, and
that you cannot have Typhoon Control Center and the webserver running
at once (only one simulation can run on the same computer).

After installing, you have to manually install the Typhoon Python API
by running (replacing the path with wherever you installed Typhoon
Control Center):

#+BEGIN_SRC shell
pip3 install C:\Program Files\Typhoon HIL Control Center 2019.3\api_install\Typhoon_HIL_API-1.7.0-py2.py3-none-any.whl
#+END_SRC

You can verify that the API install worked by opening a python3 repl and
typing:

#+BEGIN_SRC python3
import typhoon.api.hil as hil
#+END_SRC

If it succeeds, the install probably worked.

** InfluxDB
Download InfluxDB from the official website:

https://www.influxdata.com/get-influxdb/

Currently, there are only ~.exe~ files available for windows. So there
is no MSI file to install.

The ~.exe~ you will want to run after downloading InfluxDB is
~influxd.exe~ . This starts the actual database process in the
background in a ~cmd.exe~.

** Grafana
Download Grafana from the offical website:
https://grafana.com/get

For windows there should be some MSI which installs a
service. Install, and make sure the service is running. Once running
you can go to http://localhost:3000/ to see your Grafana instance.

Next, login. Then import all of the dashboards found in the ~/grafana~
directory of this repo. To import a dashboard, click on the plus sign
"+" in the Grafana UI, and select "Import". Then click "Upload .json
file". Navigate to the ~/grafana~ directory and individually upload each 
dashboard.

*** Setting up Anonymous authentication
   If it is required that Grafana should be accessed without creating
   Grafana accounts (without logging in). Updated the granfana config
   the following way:

#+BEGIN_SRC 
[auth.anonymous]
# enable anonymous access
enabled = true

# specify organization name that should be used for unauthenticated users
org_name = UWM.

# specify role for unauthenticated users
org_role = Viewer
#+END_SRC

https://grafana.com/docs/grafana/latest/auth/#anonymous-authentication

* Usage 
You can run the server in debug mode by running:
#+BEGIN_SRC sh
python3 ui.py
#+END_SRC

Then go to http://localhost:5000/ to view the server

** Streaming KPIs from Typhoon HIL
Go to the "Simulate" tab. Select a model to run (note that PV Array
and Wind Turbine have been separated into two selections for
performance reasons when computing many THDs). The "Stream KPIs"
button controls whether or not KPIs are streamed or just the values
are streams.

The way this works is that values are captured from Typhoon HIL, and
sent to an InfluxDB table called "signals". This is where all the
signal data goes. Then, there are special "*_kpi" tables (for each
component of the electrical system) which store the KPIs. For example
"pv_array_kpi" stores the PV Array KPIs. This detail isn't necessary
for running the code, as it is already handled in
the Grafana visualizations (that's where the data is pulled from).

You can only run one simulation at once (a limitation of Typhoon
HIL). So you have to cancel a simulation before you start the
next. Make sure to check the console for errors from the server. The
error handling of the server isn't seamless, you will find useful
information in the console (and often is not shown in the UI).

** Visualize
The "Visualize" tab simply is a collection of links to your local
grafana server's dashboards. For this to work you have to have
imported the Grafana dashboards in the ~/grafana~ directory (more on
that in the Grafana section).

** Loading Battery data and Running Batch KPIs
Running the battery data is a slightly tricky process.

First, go to the "Load Data" screen. Start by clicking on "Battery Charge".

Make sure to copy the "Data Start Time" (as you will need that exact
date to match in the "Battery Discharge" screen). Type in the file
path on your local computer to the battery charging data. Then make
sure the zero-indexed columns line up with the data in the CSV. Click
"Load Data" and make sure on the "Overview" page that the data loaded
(otherwise look in the console for error information).

Then go to the "Load Data" screen and click on "Battery
Discharge". Paste the date you copied from the last step for "Data
Start Time" into the "Data Start Time" for battery discharge (this
indicates an absolute time when the simulation began -- which has to
line up exactly). Enter an absolute file path to the discharging data
and ensure the zero-indexed columns match the data in the file (for
"Time", "Current", and "Voltage")

Then, go to "Batch Process". By default the time range on this page
will be the current time (UTC) and then the current time (UTC) + 1
hour. You will want to make sure that this time range includes all of
the data you just uploaded for the battery data. You can paste in the
date you used in the previous steps into the "Start Time", and for
"End Time", enter that time + however long the data is. You can enter
a long time range to be safe. For example, just put in "01/11/2020
06:08:37" as the "Start Time" and "12/12/2020 00:00:00" as the "End
Time". That will run the batch process for most of the year
of 2020. You just have to make sure the range includes the data you
uploaded.

** Issues
I have found that my code contains some bug when if you run the "Batch
Process" more than once on the same data, it causes the KPIs to become
really large numbers. I'm not sure if I fixed this or not, however, to
remedy the problem, you can go to the "Maintenance" page. Use that
page to delete the existing battery KPIs, and then retry the "Batch
Process". If that doesn't work, try deleting both the battery data and
battery KPIs, then running the batch process.

** Viewing Battery data in the Visualization
By default, the "Start Time" for the battery simulation in "Load Data"
is the current time (UTC). This means that the simulation pretends it
started now, and runs into the future. This means that you should look
into the future in the visualization to see the data. This can be done
by zooming out, or modifying the time range in the dashboard. You can
set the start time to "now - 2h" and the end time to "now + 2h". That
should give you a time range that will see the data.

* Configuration
There is a configuration file located at ~/src/config.json~ in the repo
which is for setting the InfluxDB URL and some other
configurations. They default to reasonable values (localhost). But, if
needed they could point to a remote InfluxDB server for example.

* How to update a model
If you receive a new updated Typhoon model, you should overwrite the
schematic file ~.tse~. With the new one (in the ~/typhoon~ directory of
this repo). For example overwrite ~typhoon/PV_Wind.tse~ with a new
version. Then delete the ~PV_Wind Target files~ folder. Then open the
~.tse~ file in Typhoon HIL. Click on the "Model" option in the Schematic
Editor. Select "Compile schematic". This will compile the model
(creating a new ~PV_Wind Target files~ folder). The Typhoon Python API
code that runs in the server relies on a pre-compiled model so that
the Python itself doesn't have to compile a schematic for every time
it is run. Which is why this step is necessary.
