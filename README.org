* Data Processing System (DPS)
Data Processing System (DPS) is a platform for data analytics and visualization. The platform handles ingestion of data, specifying user-defined analytics on that data, and visualizing the results. The code is design in a micro-service oriented way. As an overview, the following is a comprehnsive list of micro-serivces and their purposes:

- ~DPS UI~: The user interface for both administrators and end-users (a React app).
- ~DPS Manager~: The primary configuration server, where analytics are defined, and the backend for the user interface.
- ~DPS Database Manager~: An interface to data that has been ingested by the system (uses TimescaleDB).
- ~DPS Batch Processor~: A worker process which runs computations (any number can be running at once).
- ~DPS Relay~: An adapter service which provides an API to send data to the system (allows for scheduling when data ingestion occurs).

** Installation
There are two different ways to install the system. First, there are instructions for installing a development environment. This should be done for anyone wanting to modify the code. The second installation, is a production installation which should be used when deploying the system to servers for high performance.

Both development and production installations require the use of Docker. If you do not have Docker installed already you can find it here: https://www.docker.com/ .

*** Development Installation
The first step is to provide an initial API key that will be hard coded into the DPS Manager for built-in services to use (such as DPS Batch Processor). In the root of this repository (the main ~dps~ directory), run this command (replacing "enter secret here" with some secret text):

#+BEGIN_SRC sh
echo "API_KEY=enter secret here" > docker/secrets.env
#+END_SRC

Then to build all images and start containers run:
#+BEGIN_SRC sh
docker-compose up
#+END_SRC

This takes a while to complete.

After all containers are online, you can navigate to `http://localhost:3005` to view the UI.

*** Production Installation
The first step is to provide an initial API key that will be hard coded into the DPS Manager for built-in services to use (such as DPS Batch Processor). In the root of this repository (the main ~dps~ directory), run this command (replacing "enter secret here" with some secret text):

#+BEGIN_SRC sh
echo "API_KEY=enter secret here" > docker/secrets.env
#+END_SRC
To build all images and start containers run:

#+BEGIN_SRC sh
docker-compose -f docker-compose.yml -f docker-compose.production.yml up
#+END_SRC

For production setups you can manually run migrations and load fixtures to update your environment.

#+BEGIN_SRC sh
docker-compose exec dps_manager python manage.py migrate --noinput
docker-compose exec dps_manager python manage.py loaddata objects.json
#+END_SRC

** Development FAQ

*** How do you update the default systems?
There are default systems that are present when a new DPS Manager system is deployed. These are created via Django's "fixtures".
To update the fixtures, in DPS UI, modify your systems to have exactly the systems you would like to be present in the system on a fresh install.
Then, run the following command:

#+BEGIN_SRC sh
cd dps_manager && python3 manage.py dumpdata dps_manager_api.object --indent=2 > ./dps_manager_api/fixtures/objects.json
#+END_SRC

Now when you start a new DPS installation, it will have exactly the same objects that are in your current database. Be careful not to include things like ~AuthToken~ or ~BatchProcess~ along with this. You can manage which objects are in your database by navigating to the DPS Manager URL: ~http://dps-manager:8000/admin~, logging in with your admin credentials, and deleting any objects you don't want in the fixtures.

To manually load the fixtures (for testing), you can use this command (while in the ~dps_manager~ directory).

#+BEGIN_SRC 
python3 manage.py loaddata dps_manager_api/fixtures/objects.json
#+END_SRC

These fixtures are automatically loaded when using Docker only done once when the container is built.
